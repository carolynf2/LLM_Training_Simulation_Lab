# LLM Training Lab Configuration File
# This is the default configuration that covers all available options

project:
  name: "my_training_dataset"
  version: "1.0.0"
  description: "Dataset for fine-tuning"
  output_dir: "./outputs"
  tracking_dir: "./tracking"
  checkpoint_dir: "./checkpoints"

ingestion:
  sources:
    - type: "file"
      path: "data/raw/*.jsonl"
    - type: "web"
      urls: ["https://example.com/data"]
      delay: 1.0
      timeout: 30
    # - type: "api"
    #   api_type: "wikipedia"
    #   query: "machine learning"
    #   limit: 100
  batch_size: 1000
  max_workers: 4
  timeout: 30
  rate_limit: 1.0

preprocessing:
  cleaning:
    remove_html: true
    fix_encoding: true
    normalize_unicode: true
    normalize_whitespace: true
    remove_urls: false
    remove_emails: false
    min_length: 10
    max_length: 10000
    target_languages: ["en"]
    custom_patterns: {}

  tokenizer:
    type: "sentencepiece"  # Options: "sentencepiece", "simple"
    vocab_size: 32000
    model_type: "bpe"  # For SentencePiece: "bpe", "unigram", "char", "word"
    character_coverage: 0.9995

  deduplication:
    method: "minhash"  # Options: "exact", "minhash", "content"
    threshold: 0.9
    num_perm: 128  # For MinHash
    shingle_size: 3

quality:
  filters:
    - type: "length"
      min_length: 10
      max_length: 10000
      unit: "characters"  # Options: "characters", "words", "sentences"

    - type: "language"
      languages: ["en"]
      confidence: 0.7

    - type: "quality"
      min_score: 0.5

    - type: "toxicity"
      max_score: 0.3

    # - type: "content_type"
    #   allowed_types: ["general", "news", "academic"]
    #   min_confidence: 0.6

  min_quality_score: 0.5
  enable_validators: true

augmentation:
  enabled: true
  techniques:
    - "paraphrase"
    - "back_translation"
    # - "synthetic"
  augmentation_ratio: 0.2
  intensity: 0.3

output:
  format: "jsonl"  # Options: "jsonl", "parquet", "hf_dataset"
  path: "./outputs/processed/"
  split_ratios:
    train: 0.8
    validation: 0.1
    test: 0.1
  compression: "gzip"  # Options: "none", "gzip", "snappy" (for parquet)
  schema_mapping:
    # Map internal fields to output fields
    text: "text"
    source: "metadata.source"
    # title: "title"

monitoring:
  log_level: "INFO"  # Options: "DEBUG", "INFO", "WARNING", "ERROR"
  log_file: "./logs/llm_training_lab.log"
  log_format: "standard"  # Options: "standard", "json"
  metrics_export: true
  dashboard: false
  enable_performance_monitoring: true